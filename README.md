# ğŸ“Š Mathematics for Data Science & Machine Learning

> A comprehensive reference guide covering all essential mathematical formulas used in Data Science and Machine Learning

[![GitHub](https://img.shields.io/badge/GitHub-Repository-blue?style=flat&logo=github)](https://github.com)
[![License](https://img.shields.io/badge/License-MIT-green.svg)](LICENSE)
[![Contributions Welcome](https://img.shields.io/badge/Contributions-Welcome-brightgreen.svg)](CONTRIBUTING.md)

---

## ğŸ“‘ Table of Contents

- [Linear Algebra](#1-linear-algebra)
- [Calculus](#2-calculus)
- [Probability](#3-probability)
- [Statistics](#4-statistics)
- [Linear & Logistic Regression](#5-linear--logistic-regression)
- [Neural Networks](#6-neural-networks)
- [Optimization Algorithms](#7-optimization-algorithms)
- [Evaluation Metrics](#8-evaluation-metrics)
- [Clustering](#9-clustering)
- [Deep Learning](#10-deep-learning)
- [Dimensionality Reduction](#11-dimensionality-reduction)
- [Information Theory](#12-information-theory)
- [Support Vector Machines](#13-support-vector-machines)
- [Decision Trees & Ensembles](#14-decision-trees--ensemble-methods)
- [Bias-Variance Tradeoff](#15-bias-variance-tradeoff)

---

## 1. Linear Algebra

### ğŸ”¢ Vectors and Matrices

| Formula | Expression |
|---------|------------|
| **Dot Product** | `a Â· b = Î£(aáµ¢báµ¢) = aâ‚bâ‚ + aâ‚‚bâ‚‚ + ... + aâ‚™bâ‚™` |
| **Matrix Multiplication** | `C = AB where Cáµ¢â±¼ = Î£â‚–(Aáµ¢â‚–Bâ‚–â±¼)` |
| **Transpose** | `(AB)áµ€ = Báµ€Aáµ€` |
| **Identity Matrix** | `AI = IA = A` |
| **Inverse Matrix** | `AAâ»Â¹ = Aâ»Â¹A = I` |

### ğŸ“ Norms

| Norm Type | Formula |
|-----------|---------|
| **L1 Norm (Manhattan)** | `â€–xâ€–â‚ = Î£\|xáµ¢\|` |
| **L2 Norm (Euclidean)** | `â€–xâ€–â‚‚ = âˆš(Î£xáµ¢Â²)` |
| **Frobenius Norm** | `â€–Aâ€–F = âˆš(Î£áµ¢Î£â±¼ aáµ¢â±¼Â²)` |

### ğŸ¯ Eigenvalues and Eigenvectors

```
Eigenvalue Equation:     Av = Î»v
Characteristic Equation: det(A - Î»I) = 0
Trace:                   tr(A) = Î£Î»áµ¢ = Î£aáµ¢áµ¢
Determinant:             det(A) = Î Î»áµ¢
```

---

## 2. Calculus

### ğŸ“ Basic Derivatives

| Rule | Formula |
|------|---------|
| **Power Rule** | `d/dx(xâ¿) = nxâ¿â»Â¹` |
| **Chain Rule** | `d/dx[f(g(x))] = f'(g(x)) Â· g'(x)` |
| **Product Rule** | `d/dx[f(x)g(x)] = f'(x)g(x) + f(x)g'(x)` |
| **Quotient Rule** | `d/dx[f(x)/g(x)] = [f'(x)g(x) - f(x)g'(x)]/g(x)Â²` |

### âˆ‡ Partial Derivatives

```
Gradient:        âˆ‡f = [âˆ‚f/âˆ‚xâ‚, âˆ‚f/âˆ‚xâ‚‚, ..., âˆ‚f/âˆ‚xâ‚™]
Hessian Matrix:  Háµ¢â±¼ = âˆ‚Â²f/(âˆ‚xáµ¢âˆ‚xâ±¼)
Jacobian Matrix: Jáµ¢â±¼ = âˆ‚fáµ¢/âˆ‚xâ±¼
```

### ğŸ§® Common Activation Function Derivatives

| Function | Derivative |
|----------|------------|
| **Sigmoid** | `Ïƒ'(x) = Ïƒ(x)(1 - Ïƒ(x))` |
| **Tanh** | `tanh'(x) = 1 - tanhÂ²(x)` |
| **ReLU** | `ReLU'(x) = {1 if x > 0, 0 otherwise}` |
| **Exponential** | `d/dx(eË£) = eË£` |
| **Logarithm** | `d/dx(ln x) = 1/x` |

---

## 3. Probability

### ğŸ² Basic Probability Rules

```
Probability:              P(A) = (Favorable outcomes)/(Total outcomes)
Complement Rule:          P(Aá¶œ) = 1 - P(A)
Addition Rule:            P(A âˆª B) = P(A) + P(B) - P(A âˆ© B)
Multiplication Rule:      P(A âˆ© B) = P(A|B)P(B) = P(B|A)P(A)
Conditional Probability:  P(A|B) = P(A âˆ© B)/P(B)
```

### ğŸ”® Bayes' Theorem

```
Bayes' Rule:      P(A|B) = [P(B|A)P(A)]/P(B)

Extended Form:    P(A|B) = [P(B|A)P(A)]/[P(B|A)P(A) + P(B|Aá¶œ)P(Aá¶œ)]
```

> **ğŸ’¡ Key Application:** Fundamental in ML for classification, spam detection, and probabilistic reasoning

### ğŸ“Š Expected Value & Variance

| Concept | Formula |
|---------|---------|
| **Expected Value** | `E[X] = Î£ xáµ¢P(xáµ¢)` or `âˆ« xf(x)dx` |
| **Variance** | `Var(X) = E[(X - Î¼)Â²] = E[XÂ²] - (E[X])Â²` |
| **Standard Deviation** | `Ïƒ = âˆšVar(X)` |
| **Covariance** | `Cov(X,Y) = E[(X - Î¼â‚“)(Y - Î¼áµ§)]` |
| **Correlation** | `Ï(X,Y) = Cov(X,Y)/(Ïƒâ‚“Ïƒáµ§)` |

---

## 4. Statistics

### ğŸ“ˆ Descriptive Statistics

| Measure | Formula |
|---------|---------|
| **Mean** | `Î¼ = (1/n)Î£xáµ¢` |
| **Sample Variance** | `sÂ² = [1/(n-1)]Î£(xáµ¢ - xÌ„)Â²` |
| **Population Variance** | `ÏƒÂ² = (1/n)Î£(xáµ¢ - Î¼)Â²` |
| **Standard Error** | `SE = Ïƒ/âˆšn` |

### ğŸ¯ Hypothesis Testing

```
Z-score:             z = (x - Î¼)/Ïƒ
T-statistic:         t = (xÌ„ - Î¼)/(s/âˆšn)
Confidence Interval: CI = xÌ„ Â± z(Î±/2) Â· SE
```

**Key Terms:**
- **Type I Error (Î±):** Rejecting true null hypothesis
- **Type II Error (Î²):** Failing to reject false null hypothesis
- **Power:** 1 - Î²

---

## 5. Linear & Logistic Regression

### ğŸ“‰ Linear Regression

| Component | Formula |
|-----------|---------|
| **Simple Model** | `y = Î²â‚€ + Î²â‚x + Îµ` |
| **Matrix Form** | `y = XÎ² + Îµ` |
| **Normal Equation** | `Î² = (Xáµ€X)â»Â¹Xáµ€y` |
| **Predicted Values** | `Å· = XÎ²` |

### ğŸ“Š Loss Functions

```
Mean Squared Error (MSE):     MSE = (1/n)Î£(yáµ¢ - Å·áµ¢)Â²
Root Mean Squared Error:      RMSE = âˆšMSE
Mean Absolute Error (MAE):    MAE = (1/n)Î£|yáµ¢ - Å·áµ¢|
R-squared:                    RÂ² = 1 - [Î£(yáµ¢ - Å·áµ¢)Â²]/[Î£(yáµ¢ - È³)Â²]
Adjusted RÂ²:                  RÂ²adj = 1 - [(1 - RÂ²)(n - 1)]/(n - p - 1)
```

### ğŸ”„ Logistic Regression

| Component | Formula |
|-----------|---------|
| **Sigmoid Function** | `Ïƒ(z) = 1/(1 + eâ»á¶»)` |
| **Logit** | `z = Î²â‚€ + Î²â‚xâ‚ + Î²â‚‚xâ‚‚ + ... + Î²â‚™xâ‚™` |
| **Probability** | `P(y=1\|x) = Ïƒ(wáµ€x + b)` |
| **Odds** | `Odds = P(y=1)/P(y=0) = eá¶»` |
| **Log-Odds** | `log(Odds) = z` |

```
Log Loss (Binary Cross-Entropy):
L = -(1/n)Î£[yáµ¢log(Å·áµ¢) + (1-yáµ¢)log(1-Å·áµ¢)]
```

### ğŸ›¡ï¸ Regularization

| Type | Cost Function |
|------|---------------|
| **Ridge (L2)** | `J(Î²) = Î£(yáµ¢ - Å·áµ¢)Â² + Î»Î£Î²â±¼Â²` |
| **Lasso (L1)** | `J(Î²) = Î£(yáµ¢ - Å·áµ¢)Â² + Î»Î£\|Î²â±¼\|` |
| **Elastic Net** | `J(Î²) = Î£(yáµ¢ - Å·áµ¢)Â² + Î»â‚Î£\|Î²â±¼\| + Î»â‚‚Î£Î²â±¼Â²` |

---

## 6. Neural Networks

### âš¡ Activation Functions

| Function | Formula |
|----------|---------|
| **Sigmoid** | `Ïƒ(x) = 1/(1 + eâ»Ë£)` |
| **Tanh** | `tanh(x) = (eË£ - eâ»Ë£)/(eË£ + eâ»Ë£)` |
| **ReLU** | `ReLU(x) = max(0, x)` |
| **Leaky ReLU** | `f(x) = {x if x > 0, Î±x otherwise}` |
| **Softmax** | `softmax(xáµ¢) = eË£â±/Î£â±¼eË£Ê²` |

### ğŸ”„ Forward Propagation

```
Linear Combination:  zâ½Ë¡â¾ = Wâ½Ë¡â¾aâ½Ë¡â»Â¹â¾ + bâ½Ë¡â¾
Activation:          aâ½Ë¡â¾ = g(zâ½Ë¡â¾)
```

### â¬…ï¸ Backpropagation

```
Output Layer Error:   Î´â½á´¸â¾ = (aâ½á´¸â¾ - y) âŠ™ g'(zâ½á´¸â¾)
Hidden Layer Error:   Î´â½Ë¡â¾ = [(Wâ½Ë¡âºÂ¹â¾)áµ€Î´â½Ë¡âºÂ¹â¾] âŠ™ g'(zâ½Ë¡â¾)
Weight Gradient:      âˆ‚L/âˆ‚Wâ½Ë¡â¾ = Î´â½Ë¡â¾(aâ½Ë¡â»Â¹â¾)áµ€
Bias Gradient:        âˆ‚L/âˆ‚bâ½Ë¡â¾ = Î´â½Ë¡â¾
```

---

## 7. Optimization Algorithms

### â¬‡ï¸ Gradient Descent

| Variant | Update Rule |
|---------|-------------|
| **Batch GD** | `Î¸ := Î¸ - Î±âˆ‡J(Î¸)` |
| **Stochastic GD** | `Î¸ := Î¸ - Î±âˆ‡J(Î¸; xâ±, yâ±)` |
| **Mini-batch GD** | Uses small batches of data |

### ğŸš€ Advanced Optimizers

```
Momentum:
  v := Î²v + (1-Î²)âˆ‡J(Î¸)
  Î¸ := Î¸ - Î±v

RMSprop:
  s := Î²s + (1-Î²)(âˆ‡J)Â²
  Î¸ := Î¸ - Î±âˆ‡J/âˆš(s + Îµ)

Adam:
  m := Î²â‚m + (1-Î²â‚)âˆ‡J
  v := Î²â‚‚v + (1-Î²â‚‚)(âˆ‡J)Â²
  Î¸ := Î¸ - Î±Â·mÌ‚/âˆš(vÌ‚ + Îµ)
```

---

## 8. Evaluation Metrics

### âœ… Classification Metrics

| Metric | Formula |
|--------|---------|
| **Accuracy** | `(TP + TN)/(TP + TN + FP + FN)` |
| **Precision** | `TP/(TP + FP)` |
| **Recall (Sensitivity)** | `TP/(TP + FN)` |
| **Specificity** | `TN/(TN + FP)` |
| **F1-Score** | `2Â·(PrecisionÂ·Recall)/(Precision + Recall)` |
| **F-beta Score** | `(1 + Î²Â²)Â·(PrecisionÂ·Recall)/(Î²Â²Â·Precision + Recall)` |

### ğŸ“‰ Confusion Matrix

```
                    Predicted
                 Positive  Negative
Actual Positive     TP        FN
       Negative     FP        TN
```

**Legend:**
- **TP** = True Positive
- **TN** = True Negative
- **FP** = False Positive (Type I Error)
- **FN** = False Negative (Type II Error)

### ğŸ“ˆ ROC Curve

```
TPR (True Positive Rate):  TP/(TP + FN)
FPR (False Positive Rate): FP/(FP + TN)
AUC (Area Under Curve):    âˆ«â‚€Â¹ TPR(FPRâ»Â¹(x))dx
```

---

## 9. Clustering

### ğŸ¯ K-Means

```
Objective Function:  minimize Î£áµâ±¼â‚Œâ‚ Î£â‚“âˆˆCâ±¼ ||x - Î¼â±¼||Â²
Centroid Update:     Î¼â±¼ = (1/|Câ±¼|)Î£â‚“âˆˆCâ±¼ x
```

### ğŸ“ Distance Metrics

| Metric | Formula |
|--------|---------|
| **Euclidean** | `d(x, y) = âˆš(Î£áµ¢(xáµ¢ - yáµ¢)Â²)` |
| **Manhattan** | `d(x, y) = Î£áµ¢\|xáµ¢ - yáµ¢\|` |
| **Cosine Similarity** | `cos(Î¸) = (xÂ·y)/(â€–xâ€–Â·â€–yâ€–)` |

### ğŸ“Š Silhouette Score

```
s(i) = [b(i) - a(i)]/max{a(i), b(i)}

Where:
  a(i) = mean distance to points in same cluster
  b(i) = mean distance to points in nearest cluster
```

---

## 10. Deep Learning

### ğŸ”„ Batch Normalization

```
Normalize:        xÌ‚ = (x - Î¼_B)/âˆš(ÏƒÂ²_B + Îµ)
Scale and Shift:  y = Î³xÌ‚ + Î²
```

### ğŸ§  Convolutional Neural Networks (CNN)

```
Convolution Operation:  (f * g)(t) = Î£â‚“ f(x)g(t - x)
Output Size:            O = [(W - K + 2P)/S] + 1

Where:
  W = input size
  K = kernel size
  P = padding
  S = stride
```

### ğŸ” Recurrent Neural Networks (RNN)

```
Hidden State:  hâ‚œ = tanh(Wâ‚“â‚•xâ‚œ + Wâ‚•â‚•hâ‚œâ‚‹â‚ + bâ‚•)
Output:        yâ‚œ = Wâ‚•áµ§hâ‚œ + báµ§
```

### ğŸ§¬ Long Short-Term Memory (LSTM)

```
Forget Gate:  fâ‚œ = Ïƒ(WfÂ·[hâ‚œâ‚‹â‚, xâ‚œ] + bf)
Input Gate:   iâ‚œ = Ïƒ(Wáµ¢Â·[hâ‚œâ‚‹â‚, xâ‚œ] + báµ¢)
Output Gate:  oâ‚œ = Ïƒ(WoÂ·[hâ‚œâ‚‹â‚, xâ‚œ] + bo)
Cell State:   Câ‚œ = fâ‚œ âŠ™ Câ‚œâ‚‹â‚ + iâ‚œ âŠ™ CÌƒâ‚œ
```

### ğŸ’§ Dropout

```
Training:  Output = mask âŠ™ activation / (1 - p)
Testing:   Use all neurons (no dropout)
```

---

## 11. Dimensionality Reduction

### ğŸ“Š Principal Component Analysis (PCA)

```
Covariance Matrix:          Î£ = (1/n)Xáµ€X
Principal Components:       Eigenvectors of Î£
Explained Variance Ratio:   Î»áµ¢/Î£â±¼Î»â±¼
Projection:                 Z = XW (W = eigenvectors)
```

### ğŸ”¢ Singular Value Decomposition (SVD)

```
Decomposition:   X = UÎ£Váµ€
Reduced Form:    X â‰ˆ Uâ‚–Î£â‚–Vâ‚–áµ€
```

---

## 12. Information Theory

### ğŸ“¡ Entropy and Information

| Measure | Formula |
|---------|---------|
| **Entropy** | `H(X) = -Î£ P(xáµ¢)logâ‚‚P(xáµ¢)` |
| **Cross-Entropy** | `H(p,q) = -Î£ p(x)log q(x)` |
| **KL Divergence** | `DKL(Pâ€–Q) = Î£ P(x)log[P(x)/Q(x)]` |
| **Mutual Information** | `I(X;Y) = H(X) - H(X\|Y) = H(Y) - H(Y\|X)` |
| **Conditional Entropy** | `H(Y\|X) = -Î£â‚“Î£áµ§ P(x,y)log P(y\|x)` |

---

## 13. Support Vector Machines

### ğŸ¯ Linear SVM

```
Decision Function:  f(x) = wáµ€x + b
Margin:             2/||w||
Optimization:       minimize Â½||w||Â²
                    subject to yáµ¢(wáµ€xáµ¢ + b) â‰¥ 1
```

### ğŸ›¡ï¸ Soft Margin SVM

```
Objective:   minimize Â½||w||Â² + CÎ£Î¾áµ¢
Constraint:  yáµ¢(wáµ€xáµ¢ + b) â‰¥ 1 - Î¾áµ¢, Î¾áµ¢ â‰¥ 0
```

### ğŸ”§ Kernel Functions

| Kernel | Formula |
|--------|---------|
| **Linear** | `K(x, x') = xáµ€x'` |
| **Polynomial** | `K(x, x') = (xáµ€x' + c)áµˆ` |
| **RBF (Gaussian)** | `K(x, x') = exp(-Î³â€–x - x'â€–Â²)` |
| **Sigmoid** | `K(x, x') = tanh(Î±xáµ€x' + c)` |

---

## 14. Decision Trees & Ensemble Methods

### ğŸŒ³ Impurity Measures

| Measure | Formula |
|---------|---------|
| **Gini Impurity** | `Gini = 1 - Î£áµ¢ páµ¢Â²` |
| **Entropy** | `H = -Î£áµ¢ páµ¢logâ‚‚(páµ¢)` |
| **Classification Error** | `E = 1 - max(páµ¢)` |

```
Information Gain:  IG(D, A) = H(D) - Î£áµ¥[(|Dáµ¥|/|D|)Â·H(Dáµ¥)]
```

### ğŸŒ² Ensemble Methods

```
Bagging (Random Forest):
  Prediction: Å· = (1/B)Î£áµ‡ fáµ‡(x)

AdaBoost:
  Sample Weight:  wáµ¢â½áµ—âºÂ¹â¾ = wáµ¢â½áµ—â¾Â·exp[Î±â‚œÂ·I(yáµ¢ â‰  hâ‚œ(xáµ¢))]
  Model Weight:   Î±â‚œ = Â½ln[(1 - Îµâ‚œ)/Îµâ‚œ]

Gradient Boosting:
  Update:   Fâ‚˜(x) = Fâ‚˜â‚‹â‚(x) + Î³â‚˜hâ‚˜(x)
  Residual: ráµ¢â‚˜ = -[âˆ‚L(yáµ¢, F(xáµ¢))/âˆ‚F(xáµ¢)]
```

---

## 15. Bias-Variance Tradeoff

### âš–ï¸ Error Decomposition

```
Total Error = BiasÂ² + Variance + Irreducible Error

Bias:     Bias = E[Å·] - y
Variance: Variance = E[(Å· - E[Å·])Â²]
```

**Key Insights:**
- **High Bias** â†’ Underfitting (model too simple)
- **High Variance** â†’ Overfitting (model too complex)
- **Goal:** Find the optimal balance between bias and variance

---

## ğŸ¯ Quick Reference: Loss Functions

### Regression Losses

| Loss | Formula | Use Case |
|------|---------|----------|
| **MSE** | `L = (1/n)Î£(yáµ¢ - Å·áµ¢)Â²` | Standard regression |
| **MAE** | `L = (1/n)Î£\|yáµ¢ - Å·áµ¢\|` | Robust to outliers |
| **Huber** | `L = {Â½(y - Å·)Â² if \|y - Å·\| â‰¤ Î´, Î´\|y - Å·\| - Â½Î´Â² otherwise}` | Combines MSE & MAE |

### Classification Losses

| Loss | Formula | Use Case |
|------|---------|----------|
| **Binary Cross-Entropy** | `L = -[ylog(Å·) + (1-y)log(1-Å·)]` | Binary classification |
| **Categorical Cross-Entropy** | `L = -Î£áµ¢ yáµ¢log(Å·áµ¢)` | Multi-class classification |
| **Hinge Loss** | `L = max(0, 1 - yÂ·Å·)` | SVM classification |

---

## ğŸ“ Feature Engineering

### Normalization Techniques

| Method | Formula | Range |
|--------|---------|-------|
| **Min-Max Scaling** | `x' = (x - min)/(max - min)` | [0, 1] |
| **Z-Score Normalization** | `x' = (x - Î¼)/Ïƒ` | ~ [-3, 3] |
| **Max Abs Scaling** | `x' = x/\|max\|` | [-1, 1] |

### Polynomial Features

```
Degree 2: [xâ‚, xâ‚‚] â†’ [1, xâ‚, xâ‚‚, xâ‚Â², xâ‚xâ‚‚, xâ‚‚Â²]
Degree 3: [xâ‚, xâ‚‚] â†’ [1, xâ‚, xâ‚‚, xâ‚Â², xâ‚xâ‚‚, xâ‚‚Â², xâ‚Â³, xâ‚Â²xâ‚‚, xâ‚xâ‚‚Â², xâ‚‚Â³]
```

---

## ğŸŒŸ Contributing

Contributions are welcome! If you find any errors or want to add more formulas:

1. Fork the repository
2. Create a new branch (`git checkout -b feature/new-formulas`)
3. Make your changes
4. Commit your changes (`git commit -am 'Add new formulas'`)
5. Push to the branch (`git push origin feature/new-formulas`)
6. Create a Pull Request

---

## ğŸ“š Resources

- [Mathematics for Machine Learning Book](https://mml-book.github.io/)
- [Deep Learning Book](https://www.deeplearningbook.org/)
- [Pattern Recognition and Machine Learning](https://www.microsoft.com/en-us/research/publication/pattern-recognition-machine-learning/)
- [Scikit-learn Documentation](https://scikit-learn.org/)
- [TensorFlow Documentation](https://www.tensorflow.org/)
- [PyTorch Documentation](https://pytorch.org/)

---

## ğŸ“ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

---

## ğŸ’¡ Tips for Using This Guide

1. **Bookmark** this page for quick reference during ML projects
2. **Print** specific sections you use frequently
3. **Practice** implementing these formulas in code
4. **Understand** the intuition behind each formula, not just memorization
5. **Share** with fellow ML practitioners and students

---

## ğŸ¤ Acknowledgments

Created with â¤ï¸ for the Machine Learning community. Special thanks to all contributors and the open-source ML community.

---

## â­ Star This Repo!

If you find this helpful, please consider giving it a star! It helps others discover this resource.

---

## ğŸ‘¥ Contributors

<div align="center">

### Created and Maintained by

<a href="https://github.com/iNSRawat">
  <img src="https://github.com/iNSRawat.png" width="100" height="100" style="border-radius: 50%;" alt="iNSRawat"/>
</a>

**[@iNSRawat](https://github.com/iNSRawat)**

*Creator & Maintainer*

---

### ğŸ¤ Want to Contribute?

We welcome contributions! If you have:
- Formula corrections or additions
- Documentation improvements
- Feature suggestions
- Bug reports

Please feel free to:
1. Fork the repository
2. Make your changes
3. Submit a pull request

---

### ğŸ“¬ Connect

- ğŸ’¼ **LinkedIn**: [Connect with me](https://linkedin.com/in/insrawat)
- ğŸ¦ **Twitter**: [@iNSRawat](https://twitter.com/insrawat)
- ğŸ“§ **GitHub**: [Open an issue](https://github.com/iNSRawat/Mathematics-Formulas-for-Data-Science-Machine-Learning/issues)

---

<div align="center">

â­ **If this project helped you, please give it a star!** â­

Made with â¤ï¸ by [@iNSRawat](https://github.com/iNSRawat)

</div>

</div>

---

**Happy Learning! ğŸš€ğŸ“ŠğŸ¤–**
